{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff23b49-902f-4628-a557-b99b4c08cd3c",
   "metadata": {},
   "source": [
    "#  توضیحات این کد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c82765-ebd2-4519-b301-232a63f46da7",
   "metadata": {},
   "source": [
    "## فراخوان کردن کتابخانه‌های لازم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5fdaa00-c3ea-4bd8-9c6b-4893479a3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log #  To use logarithm calculation in Python_برای استفاده از محاسبه‌ی لوگاریتم در پایتون\n",
    "from numpy import astype,array # To convert a list to an unordered list and change the data format_برای تبدیل کردن لیست به لیست نامپایی و تغییر فرمت دیتا\n",
    "import scipy.stats as sss # To calculate Shannon entropy_برای محاسبه‌ی انتروپی شنون"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f001935-7e7c-454f-810b-21112b39af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence by Manual Calculation :  4.4594709547005404\n",
      "\n",
      "KL Divergence by Entropy Function :  4.4594709547005404\n",
      "\n",
      "Shannon Entropy by Manual Calculation :  [0.5297062  0.03238875 0.13579396 0.39031313 0.65548177 0.202273\n",
      " 0.06359604 0.63008614 0.03953686 0.117535   0.202273   0.0736038 ]\n",
      "\n",
      "Shannon Entropy by Entropy Function :  [0.5297062  0.03238875 0.13579396 0.39031313 0.65548177 0.202273\n",
      " 0.06359604 0.63008614 0.03953686 0.117535   0.202273   0.0736038 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1= [2,3,1,7,4,4,5,75,3,66,74,423];x2=[7,577,32,46,7,74,423,36,454,2556,4,6] #datas are random_دیتا ها تصادفی هستند\n",
    "anskl = 0\n",
    "ansse = []\n",
    "if len(x1)!=len(x2):\n",
    "    print(\"ERROR! x1 & x2 must have same length.\")\n",
    "else:\n",
    "    for i in range(len(x1)):\n",
    "        anskl+=x1[i]/sum(x1)*log((x1[i]/sum(x1))/(x2[i]/sum(x2)))\n",
    "        ansse.append(-((x1[i]/(x1[i]+x2[i]))*log(x1[i]/(x1[i]+x2[i]))+(x2[i]/(x1[i]+x2[i]))*log(x2[i]/(x1[i]+x2[i]))))\n",
    "    print(\"KL Divergence by Manual Calculation : \",anskl,end=\"\\n\\n\")\n",
    "    print(\"KL Divergence by Entropy Function : \",sss.entropy(x1,x2),end=\"\\n\\n\")\n",
    "    print(\"Shannon Entropy by Manual Calculation : \",array(ansse).astype(float),end=\"\\n\\n\")\n",
    "    print(\"Shannon Entropy by Entropy Function : \",sss.entropy([x1,x2]),end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
